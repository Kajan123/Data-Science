{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bllawCTaGnQa"
      },
      "source": [
        "This is the beginning of the notebook.\n",
        "\n",
        "##ADM4142-A Fundamentals of Data science <br>\n",
        "The goal of this notebook is to retrieve and stage the source datasets into the format used in the dimensional model for analysis.\n",
        "\n",
        "This notebook generates the Economy_dimension of the weather/tourism/economy data frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCVUc1nSatxu"
      },
      "source": [
        "Code below gets the data from the datasets/urls cand puts them in the database, just need to update the urls before using it or else you might get an error cuz the urls will expire. Only missing mean tempurature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H6zILMfue8j"
      },
      "source": [
        "Iterate over URLs and create the database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1oiB-OLzzE6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqxEgsKNz0fc"
      },
      "outputs": [],
      "source": [
        "# Load the date dimension with a focus on the entries for the 1st day of each month between 1990 and 2023\n",
        "date_url = 'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/dimension/date.csv'\n",
        "date_df = pd.read_csv(date_url)\n",
        "# Ensure we're focusing on entries from 1990-2023 and the 1st day of each month\n",
        "date_df_filtered = date_df[(date_df['year'].between(1990, 2023)) & (date_df['day'] == 1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAQftpNX04q4"
      },
      "outputs": [],
      "source": [
        "# List of URLs for weather data\n",
        "urls = [\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_banff_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_calgary_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_edmonton_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_montreal_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_ottawa_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_quebec_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_toronto_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_vancouver_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_victoria_normal_monthly.csv',\n",
        "    'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/data/weatherstats_whistler_normal_monthly.csv',\n",
        "    # Add more URLs here as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81Of7jGez53k",
        "outputId": "b9197276-d2a0-4dd8-8431-25898ee8166b"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty DataFrame to store concatenated results\n",
        "result_df = pd.DataFrame()\n",
        "\n",
        "# Iterate through each URL to process and transform the data\n",
        "for url in urls:\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(url)\n",
        "\n",
        "    # Extract location from the filename in URL\n",
        "    city = url.split('/')[-1].split('_')[1]\n",
        "\n",
        "    # Convert 'date' column to datetime format to extract year and month\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # Filter data for years 1990 to 2023\n",
        "    df = df[df['date'].dt.year.between(1990, 2023)]\n",
        "\n",
        "    # Calculate average temperature\n",
        "    df['avg_temperature_v'] = (df['max_temperature_v'] + df['min_temperature_v']) / 2\n",
        "\n",
        "    # Fill missing values\n",
        "    #df.fillna(df.mean(), inplace=True)\n",
        "\n",
        "    # Add 'location' column\n",
        "    df['location'] = city\n",
        "\n",
        "    # Join with the date dimension on year and month for the 1st day of each month\n",
        "    df = df.merge(date_df_filtered, left_on=[df['date'].dt.year, df['date'].dt.month], right_on=['year', 'month'], how='left')\n",
        "\n",
        "    # Keep only the required columns and rename them accordingly\n",
        "    df = df[['Date_key', 'date', 'max_temperature_v', 'min_temperature_v', 'avg_temperature_v', 'precipitation_v', 'snow_v', 'location']]\n",
        "\n",
        "    # Append the processed DataFrame to the result\n",
        "    result_df = pd.concat([result_df, df], ignore_index=True)\n",
        "\n",
        "\n",
        "# Add a placeholder for 'Location_key' (to be filled later)\n",
        "result_df['Location_key'] = ''\n",
        "\n",
        "# Sorting by location and date (oldest to newest)\n",
        "result_df.sort_values(by=['location', 'Date_key'], inplace=True)\n",
        "\n",
        "# Reset weather key to start from beginning\n",
        "result_df.reset_index(drop=True, inplace=True)\n",
        "# Add a 'Weather_key' as an enumeration from 1\n",
        "#result_df['Weather_key'] = result_df.index + 1\n",
        "result_df.insert(0, 'Weather_key', range(1, 1 + len(result_df)))\n",
        "\n",
        "# Final column order adjustment\n",
        "final_columns = ['Weather_key', 'Location_key', 'Date_key', 'date', 'max_temperature_v', 'min_temperature_v', 'avg_temperature_v', 'precipitation_v', 'snow_v', 'location']\n",
        "result_df = result_df[final_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "oE58YMcl5pE1",
        "outputId": "2d29d2c2-c790-4edb-f39f-92df942053cc"
      },
      "outputs": [],
      "source": [
        "result_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8h_6xpd5Xkk"
      },
      "source": [
        "Populate the Location_key column with the corresponding province associated to the city in 'location'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHnqow-z5eT8"
      },
      "outputs": [],
      "source": [
        "# Load the location dimension if not already loaded\n",
        "location_url = 'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/dimension/location.csv'\n",
        "location_df = pd.read_csv(location_url)\n",
        "\n",
        "# Load the date dimension if not already loaded\n",
        "date_url = 'https://raw.githubusercontent.com/noobstang/cscsi4142-project-datasets/master/dimension/date.csv'\n",
        "date_df = pd.read_csv(date_url)\n",
        "\n",
        "# Filter date_df for entries that correspond to January 1st of each year to simplify the join\n",
        "date_df_filtered = date_df[date_df['day'] == 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ulta4Ggy5f6T"
      },
      "outputs": [],
      "source": [
        "# Mapping from city to province\n",
        "city_to_province = {\n",
        "    \"banff\": \"Alberta\",\n",
        "    \"calgary\": \"Alberta\",\n",
        "    \"edmonton\": \"Alberta\",\n",
        "    \"montreal\": \"Quebec\",\n",
        "    \"quebec\": \"Quebec\",\n",
        "    \"ottawa\": \"Ontario\",\n",
        "    \"toronto\": \"Ontario\",\n",
        "    \"vancouver\": \"British Columbia\",\n",
        "    \"victoria\": \"British Columbia\",\n",
        "    \"whistler\": \"British Columbia\"\n",
        "}\n",
        "\n",
        "# Replace city names in weather_df with their corresponding province names\n",
        "result_df['location_province'] = result_df['location'].map(city_to_province)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "WABx536V6U2L",
        "outputId": "8a139211-a076-44c0-b529-850669deabda"
      },
      "outputs": [],
      "source": [
        "result_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WSA5-6Dgy4ab",
        "outputId": "7b66e600-89ef-4f39-f517-889194c4f4e6"
      },
      "outputs": [],
      "source": [
        "date_df_filtered.head(50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CuZhn7-V0_N0",
        "outputId": "2291d455-520a-4b2c-9090-f4dd4bf90983"
      },
      "outputs": [],
      "source": [
        "location_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYPiEJ0d5ew2"
      },
      "outputs": [],
      "source": [
        "# Before joining, ensure that 'location' in location_df refers to the province. If not, you may need to adjust location_df accordingly.\n",
        "\n",
        "# Make a copy of the filtered DataFrame to avoid SettingWithCopyWarning\n",
        "date_df_filtered = date_df_filtered.copy()\n",
        "\n",
        "# Convert 'date_iso' column to datetime and extract the year, correctly using a copy to avoid the warning\n",
        "#date_df_filtered['year'] = pd.to_datetime(date_df_filtered['date_iso']).dt.year\n",
        "\n",
        "# Extract the year for each Date_key in date_df for the join\n",
        "#date_df_filtered['year'] = pd.to_datetime(date_df_filtered['date_iso']).dt.year\n",
        "date_df_filtered['date_iso'] = pd.to_datetime(date_df_filtered['date_iso'])\n",
        "\n",
        "#result_df['date'] = pd.to_datetime(result_df['date'])\n",
        "result_df['year'] = pd.to_datetime(result_df['date']).dt.year\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sFWh_yZ81fU2",
        "outputId": "55624185-1ad3-425b-9665-2bf4b43c88a8"
      },
      "outputs": [],
      "source": [
        "# add year column to location\n",
        "#location_df[year] =\n",
        "location_df = location_df.copy()\n",
        "\n",
        "#location_df = location_df.merge(date_df_filtered[['year']], left_on=['Date_key'], right_on=['Date_key'], how='left')\n",
        "\n",
        "# Merge the DataFrames using the \"Date_key\" attribute\n",
        "location_merged_df = pd.merge(location_df, date_df[['Date_key', 'year']], on='Date_key', how='left')\n",
        "\n",
        "location_merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "PKGUGdyB5nIH",
        "outputId": "ea9aca5d-e053-4465-dc93-57e49975f2bc"
      },
      "outputs": [],
      "source": [
        "# Convert 'year' columns to pandas datetime format\n",
        "#result_df['year'] = pd.to_datetime(result_df['year'], format='%Y')\n",
        "#location_merged_df['year'] = pd.to_datetime(location_merged_df['year'], format='%Y')\n",
        "\n",
        "result_df.head()\n",
        "#location_merged_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "R2_jlqrO1e8_",
        "outputId": "82b7a237-3c00-4a48-f495-90130f740333"
      },
      "outputs": [],
      "source": [
        "# Merge the DataFrames using the specified attributes\n",
        "merged_temp_df = pd.merge(result_df, location_merged_df, left_on=['location_province', 'year'], right_on=['location', 'year'], how='left')\n",
        "\n",
        "\n",
        "# Perform the join with location_df to get the Location_key, matching based on 'location' and 'year'\n",
        "#location_merged_df['year'] = pd.to_datetime(location_merged_df['year'])\n",
        "\n",
        "#result_df_merge = pd.merge(result_df, location_merged_df[['Location_key']], left_on=['location_province', 'year'], right_on=['location', 'year'], how='left')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Perform the join with date_df to maintain the Date_key for each month\n",
        "#result_df = result_df.merge(date_df_filtered[['Date_key', 'date_iso', 'year']], left_on=['date', 'year'], right_on=['date_iso', 'year'], how='left')\n",
        "\n",
        "# Join weather data with date_df_filtered to get a simplified 'Date_key' that corresponds to January 1st of each year\n",
        "#result_df = result_df.merge(date_df_filtered[['year', 'Date_key']], on='year', how='left')\n",
        "\n",
        "# Join on 'location' to get the 'Location_key', using 'Date_key' as an additional join condition if necessary\n",
        "# This assumes that 'location' in location_df is already set to provinces and is compatible with our mapping\n",
        "#result_df = result_df.merge(location_df[['Location_key', 'location', 'Date_key']], left_on=['location_province', 'Date_key'], right_on=['location', 'Date_key'], how='left')\n",
        "merged_temp_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IT857Pa76ojL",
        "outputId": "967581cf-0668-4bda-c6ec-52905c6d6ed3"
      },
      "outputs": [],
      "source": [
        "# Overwrite the values of the 'Location_key' column in result_df\n",
        "result_df['Location_key'] = merged_temp_df['Location_key_y']\n",
        "\n",
        "\n",
        "result_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWKy5CyR5wsy"
      },
      "outputs": [],
      "source": [
        "# Select and rename the columns as per the final requirement\n",
        "final_weather_df = result_df[['Weather_key', 'Location_key', 'Date_key', 'max_temperature_v', 'min_temperature_v', 'avg_temperature_v', 'precipitation_v', 'snow_v', 'location']].copy()\n",
        "\n",
        "# Export the final DataFrame\n",
        "#final_weather_df.to_csv('weather_final.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6R3QI4JtpTP"
      },
      "source": [
        "Test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        },
        "id": "dFbT-x4boyoX",
        "outputId": "e5441463-53c7-4538-f79d-5a91e5f6d04d"
      },
      "outputs": [],
      "source": [
        "final_weather_df.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T59PUtQI7sgH",
        "outputId": "4ed2ceb3-7c8b-4104-b71c-2573a2e6efe0"
      },
      "outputs": [],
      "source": [
        "# Assuming df is your DataFrame\n",
        "filtered_df = location_merged_df[location_merged_df['Location_key'] >= 60]\n",
        "\n",
        "# Get the first 10 entries\n",
        "first_10_entries = filtered_df.head(50)\n",
        "\n",
        "# Display the first 10 entries\n",
        "print(first_10_entries)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiDl5j-Y80Zg"
      },
      "source": [
        "Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0WDMeZsdgGp"
      },
      "outputs": [],
      "source": [
        "# Export the final DataFrame\n",
        "#final_weather_df.to_csv('weather_final.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
